{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wQ5bDWTKl53v"
   },
   "source": [
    "# Credit Card Fraud Detection with TensorFlow\n",
    "- The credit card dataset contains transaction data used for fraud detection. It includes features like time, transaction amount, and anonymized features (V1-V28).\n",
    "- The dataset has imbalanced classes, with mostly legitimate transactions (Class 0) and fewer fraudulent ones (Class 1).\n",
    "- My goal is to build a model that accurately classify transactions as fraudulent or legitimate, using techniques like regularization and evaluation metrics such as precision, recall, and F1-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I_vNKW7frB1U",
    "outputId": "4c52fd81-9700-43fa-b155-2dea317244cb"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j7LVIOa8l533",
    "outputId": "625ad6ef-2143-4385-9dbb-c83673e5b358"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from tensorflow.keras import regularizers\n",
    "from scipy.stats import ks_2samp\n",
    "# load data\n",
    "data = pd.read_csv(\"/content/drive/MyDrive/creditcard.csv\")\n",
    "X = data.drop('Class', axis=1).values\n",
    "y = data['Class'].values\n",
    "print(X,y)\n",
    "# plot the class distribution\n",
    "pd.value_counts(data['Class'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oB242bgil539"
   },
   "source": [
    "# Distribution of credit card transactions over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 607
    },
    "id": "FipPlhvul539",
    "outputId": "c36585ea-0612-411a-c939-433e6e134a35"
   },
   "outputs": [],
   "source": [
    "# Set the aesthetic style of the plots\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "f, (ax1, ax2) = plt.subplots(2, 1, sharex=True, figsize=(12, 6))\n",
    "\n",
    "bins = 100\n",
    "\n",
    "# Plotting for Fraud transactions\n",
    "ax1.hist(data.Time[data.Class == 1], bins=bins, color='red', alpha=0.7)\n",
    "ax1.set_title('Fraud', fontsize=14)\n",
    "ax1.set_ylabel('Number of Transactions', fontsize=12)\n",
    "ax1.grid(True, linestyle='--', alpha=0.5)\n",
    "\n",
    "# Plotting for Normal transactions\n",
    "ax2.hist(data.Time[data.Class == 0], bins=bins, color='blue', alpha=0.7)\n",
    "ax2.set_title('Normal', fontsize=14)\n",
    "ax2.set_xlabel('Time (in Seconds)', fontsize=12)\n",
    "ax2.set_ylabel('Number of Transactions', fontsize=12)\n",
    "ax2.grid(True, linestyle='--', alpha=0.5)\n",
    "\n",
    "# Remove top and right spines\n",
    "for ax in [ax1, ax2]:\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xWPyYzrEl53_"
   },
   "source": [
    "The red histogram represents fraudulent transactions. The x-axis shows the time in seconds, and the y-axis represents the number of transactions. Fraudulent transactions are relatively infrequent compared to normal transactions, as seen by the lower y-axis values. There are peaks at certain times, suggesting that fraudulent activity may occur in bursts or at specific moments rather than evenly distributed throughout the time frame covered.\n",
    "\n",
    "The blue histogram represents normal transactions which have a higher volume. The pattern here is more consistent, with what appears to be a cyclical pattern suggesting higher transaction volumes at regular intervals. This could correspond to peak transaction times during the day, such as morning and evening hours when people are more likely to use their credit cards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 545
    },
    "id": "9Z54LGCyl53_",
    "outputId": "85ac6d74-b8c6-460d-e5fc-54c0f6348900"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.boxplot([data[data[\"Class\"]==1][\"Amount\"], data[data[\"Class\"]==0][\"Amount\"]],\n",
    "            labels=['Fraud', 'Normal'])\n",
    "\n",
    "plt.title('Transaction Amounts: Fraud vs Normal')\n",
    "plt.ylabel('Amount')\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JQXONtpel54A"
   },
   "source": [
    "The median fraudulent transaction is of **9.25** with the largest amount in a fraudulent transaction being **2125.87**.\n",
    "The median amount for a normal transaction is **22** whereas the largest amount is **25691** which is substantially higher than the maximum for a fraudulent transaction. This could be due to the larger sample size or it could indicate that fraudsters avoid extremely large transactions that might trigger security mechanisms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9HQ23fzdl54B"
   },
   "source": [
    "# Data Preparation\n",
    "- Split data into training, validation, and test sets (60/20/20).\n",
    "- Use stratified sampling to maintain the class distribution in each split.\n",
    "- Scale the training, validation, and test sets to a mean of zero mean and unit variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "08LFvrInl54B"
   },
   "outputs": [],
   "source": [
    "# split data into train, validation, and test\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.4, random_state=42, stratify=y)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.25, random_state=42, stratify=y_temp)\n",
    "\n",
    "# normalize data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8oLE0lLsl54C"
   },
   "source": [
    "# Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z-sKXEbyl54D",
    "outputId": "e8e29f26-a2b5-4f45-f42c-346a641581ac"
   },
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential(\n",
    "    [\n",
    "        # Adjusted number of neurons\n",
    "        tf.keras.layers.Dense(128, activation=\"relu\", input_shape=(X.shape[-1],),\n",
    "                              kernel_regularizer=regularizers.l2(0.001)),  # L2 regularization\n",
    "        tf.keras.layers.Dropout(0.2),  # Adjusted dropout rate\n",
    "        tf.keras.layers.Dense(64, activation=\"relu\",\n",
    "                              kernel_regularizer=regularizers.l2(0.001)),  # L2 regularization\n",
    "        tf.keras.layers.Dropout(0.2),  # Adjusted dropout rate\n",
    "        tf.keras.layers.Dense(1, activation=\"sigmoid\"),\n",
    "    ]\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qpUSF8Xil54E"
   },
   "source": [
    "1. Input Layer: A dense layer with 128 neurons, ReLU activation function, and L2 regularization with a strength of 0.001. It takes input data with a shape determined by the number of features in the dataset.\n",
    "2. Dropout Layer: A dropout layer with a dropout rate of 0.2, which helps prevent overfitting by randomly deactivating 20% of neurons during training.\n",
    "3. Hidden Layer: Another dense layer with 64 neurons, ReLU activation, and L2 regularization with a strength of 0.001.\n",
    "4. Another Dropout Layer: Similar to the previous dropout layer, with a rate of 0.2.\n",
    "5. Output Layer: A dense layer with a single neuron and sigmoid activation function, used for binary classification.\n",
    "\n",
    "### ReLU activation function\n",
    "- Decides whether the neuron should be activated or not.\n",
    "- Introduces non-linear transformation making it capable to learn and perform more complex tasks.\n",
    "- Maps any number to zero if it is negative, and otherwise maps it to itself.\n",
    "![image.png](attachment:cb1f20be-375c-4c90-a142-84521cd8fa33.png)\n",
    "\n",
    "### Regularization\n",
    "These are the most common regularization techniques.\n",
    "![image.png](attachment:d8711229-bbe5-46f4-87ad-c856fa8333ba.png)\n",
    "- During the model definition I introduced parameter regularization (weight decay) and neural network regularization (dropout).\n",
    "- During the model training I added early stopping.\n",
    "\n",
    "#### Weight decay\n",
    "\n",
    "Weight decay is a regularization term implemented from tensorflow.keras with the following formula:\n",
    "\n",
    "![image.png](attachment:fb3631a6-ab49-47e0-8280-eb5a62e4c249.png)\n",
    "`math_ops.reduce_sum(math_ops.square(x))` calculates the sum of the squares of all elements in the tensor x\n",
    "\n",
    "#### Dropout regularization\n",
    "- Deep neural nets with a large number of parameters are powerful but prone to overfitting.\n",
    "- Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much.\n",
    "- During training, dropout samples form an exponential number of different “thinned” networks. At test time, instead of using all these thinned networks, we can approximate their effect by using a single unthinned network with smaller weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wmORqOnpl54G"
   },
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "15vxq9BXl54H"
   },
   "outputs": [],
   "source": [
    "metrics = [\n",
    "    tf.keras.metrics.FalseNegatives(name=\"fn\"),\n",
    "    tf.keras.metrics.FalsePositives(name=\"fp\"),\n",
    "    tf.keras.metrics.TrueNegatives(name=\"tn\"),\n",
    "    tf.keras.metrics.TruePositives(name=\"tp\"),\n",
    "    tf.keras.metrics.Precision(name=\"precision\"),\n",
    "    tf.keras.metrics.Recall(name=\"recall\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GOPwE8rcl54H"
   },
   "source": [
    "In this imbalanced dataset using accuracy as the evaluation metric is not appropriate because it can be misleading. A model that predicts all instances as negative 0 would still achieve a high accuracy of 99%, even though it's not providing meaningful results. This is why I am prioritizing the following metrics:\n",
    "\n",
    "![image.png](attachment:3b1dc5fb-9d61-48f3-9666-63a12a9b4cd7.png)\n",
    "\n",
    "- `Precision` calculates the ratio of true positives to the total number of positive predictions (true positives + false positives).\n",
    "\n",
    "- `Recall` calculates the ratio of true positives to the total number of actual positive instances (true positives + false negatives)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UR7frO7il54I"
   },
   "source": [
    "# Model training\n",
    "- Use the Adam optimizer with a learning rate of 0.0001.\n",
    "- Utilize binary cross-entropy as the loss function.\n",
    "- Set up an early stopping callback that monitors the validation loss for minimization.\n",
    "- Specify a patience of 5 epochs before stopping training if the validation loss does not improve.\n",
    "- Set class weights such that class 0 has a weight of 1, and class 1 has a weight of 5, giving more importance to the minority class.\n",
    "- Train for a maximum of 100 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true,
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8FhC_py-l54I",
    "outputId": "93b5971a-aed6-4309-8e23-40a1fb4230ef"
   },
   "outputs": [],
   "source": [
    "# compile the model\n",
    "model.compile(optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=0.0001), loss='binary_crossentropy', metrics = metrics)\n",
    "\n",
    "# configure early stopping\n",
    "es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)\n",
    "\n",
    "# calculate class weights\n",
    "neg, pos = np.bincount(y_train)\n",
    "total = neg + pos\n",
    "class_weight = {0: 1, 1: 5}\n",
    "\n",
    "# train the model\n",
    "history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=100, callbacks=[es], class_weight=class_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YB9w7b6nl54J"
   },
   "source": [
    "# Model Evaluation and Loss Visualization\n",
    "- Use the trained model to make predictions on the test dataset.\n",
    "- Calculate precision, recall, and F1 score as evaluation metrics for the model's performance on the test data.\n",
    "- Visualize the training and validation loss over the training epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 692
    },
    "id": "4BD6DQ5Yl54K",
    "outputId": "c67056ca-559d-44b5-83a4-b03f11fe7a0b"
   },
   "outputs": [],
   "source": [
    "# predict test data\n",
    "y_pred = (model.predict(X_test) > 0.5).astype(\"int32\")\n",
    "\n",
    "# score precision, recall, and f1\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(f'Precision: {precision}')\n",
    "print(f'Recall: {recall}')\n",
    "print(f'F1 Score: {f1}')\n",
    "\n",
    "# Plot only the losses from history\n",
    "losses = history.history['loss']\n",
    "val_losses = history.history['val_loss']\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.plot(losses, label='Training Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e0BBY-zKl54L"
   },
   "source": [
    "- The training Loss decreases sharply at the beginning, which indicates that the model is quickly learning from the training data. As epochs increase, the rate of decrease slows down, suggesting that the model is starting to converge and is learning less from the training data with each epoch.\n",
    "- The validation Loss decreases along with the training loss, but it starts to plateau toward the end. The fact that the validation loss levels off but does not increase indicates that the model is not overfitting.\n",
    "- The early stopping after 30 epochs suggests that the model reached an optimal state in terms of generalization before performance on the validation set could deteriorate."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
